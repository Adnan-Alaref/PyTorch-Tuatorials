{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyP7Rxpin2oLl5JF3yMW+eyj"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Date Published: Decemper 17, 2024 ,\nAuthor: Adnan Alare**f","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:41:31.811705Z","iopub.execute_input":"2024-12-17T21:41:31.812121Z","iopub.status.idle":"2024-12-17T21:41:35.126668Z","shell.execute_reply.started":"2024-12-17T21:41:31.812083Z","shell.execute_reply":"2024-12-17T21:41:35.125047Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(torch.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtQyAYjgQ4jB","executionInfo":{"status":"ok","timestamp":1732317420256,"user_tz":-120,"elapsed":3,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"fd82db71-438a-414f-b5ba-f6fc124f23f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.5.1+cu121\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Introduction to Tensors.","metadata":{"id":"zHkaWdcUVddP"}},{"cell_type":"markdown","source":"## üî∏ Create scaler tensor.","metadata":{"id":"tWcvUxyXG-Cs"}},{"cell_type":"code","source":"scaler = torch.tensor(6)\nscaler, scaler.ndim , scaler.shape ,scaler.item()\n\n# Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number.","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XG9nxx1HGY9L","executionInfo":{"status":"ok","timestamp":1734461037415,"user_tz":-120,"elapsed":298,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"a13c2b26-1c87-46c4-f9f4-ec31792d7d93"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(6), 0, torch.Size([]), 6)"]},"metadata":{},"execution_count":6}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Create vector tensor.","metadata":{"id":"TwU2PWq5HIRd"}},{"cell_type":"code","source":"vector = torch.tensor([7,8])\nvector ,vector[0] ,vector[1] ,vector.ndim, vector.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzN5RkPhHHMa","executionInfo":{"status":"ok","timestamp":1734460941004,"user_tz":-120,"elapsed":335,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"1e074c9d-d460-407e-9db0-9246729ba0ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([7, 8]), tensor(7), tensor(8), 1, torch.Size([2]))"]},"metadata":{},"execution_count":3}],"execution_count":null},{"cell_type":"markdown","source":"**Note:** If you want to convert a specific element in the vector to a number, you can index the tensor and call .item()  \n\n* **.item()** is necessary to extract the scalar value as a Python number.  \n\n* Ensure that the reduction operation results in a tensor with a single element before calling **.item()**.","metadata":{"id":"wT2RNaVp8k9D"}},{"cell_type":"code","source":"# Convert the second element to a scalar\nvector[1].item() # Output: 8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZdBVL577Z33","executionInfo":{"status":"ok","timestamp":1734461327714,"user_tz":-120,"elapsed":291,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"f1f5bc44-a1dc-4358-ef2d-4a3f5c2c2a24"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":8}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Create Matrix tensor.","metadata":{"id":"PGPJ6DJQH1m8"}},{"cell_type":"code","source":"matrix = torch.tensor([[1,2,5],[3,4,9],[5,6,7]])\nmatrix ,matrix.ndim , matrix.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZG9Z1SlH7YT","executionInfo":{"status":"ok","timestamp":1734461695144,"user_tz":-120,"elapsed":313,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"dc6bbaba-991a-434b-a008-c43ad161d820"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1, 2, 5],\n","         [3, 4, 9],\n","         [5, 6, 7]]),\n"," 2,\n"," torch.Size([3, 3]))"]},"metadata":{},"execution_count":19}],"execution_count":null},{"cell_type":"code","source":"# Convert the element in [1][2] to a scalar\nmatrix[1][2].item() # Output: 9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mdw-rt9-Nby","executionInfo":{"status":"ok","timestamp":1734461698568,"user_tz":-120,"elapsed":353,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"26d26cc3-0db8-497c-ef72-a0cf2e3c7f86"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":20}],"execution_count":null},{"cell_type":"code","source":"tensor_3d = torch.tensor([[[1,2,3],[-1,5,6],[17,8,9]]])\ntensor_3d , tensor_3d.ndim , tensor_3d.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KDvVhZ45P9Ws","executionInfo":{"status":"ok","timestamp":1734462302843,"user_tz":-120,"elapsed":644,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"5fa2525a-2d7b-4dee-f140-720ab2afb3ef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[ 1,  2,  3],\n","          [-1,  5,  6],\n","          [17,  8,  9]]]),\n"," 3,\n"," torch.Size([1, 3, 3]))"]},"metadata":{},"execution_count":27}],"execution_count":null},{"cell_type":"code","source":"# To access the element at position (0, 0, 1) by use .item()\ntensor_3d[0,0,1].item() # Output: 2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlMTiiYR_mpX","executionInfo":{"status":"ok","timestamp":1734462307867,"user_tz":-120,"elapsed":3,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"8bd104db-6255-4d66-c2e9-dc747c00157f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":28}],"execution_count":null},{"cell_type":"markdown","source":"**Note:** use `.flatten()` if you want to treat the 3D tensor as a 1D vector for operations like sorting or indexing.","metadata":{"id":"smWtjElgAJhW"}},{"cell_type":"code","source":"flatten_tensor = tensor_3d.flatten()\nprint(f\"Flatten Tensor : {flatten_tensor}\") # Output: [1,2,3,-1,5,6,17,8,9]\n\n# Accessing the first element of the flattened tensor\nfirst_element = flatten_tensor[0].item()\n\nprint(f\"First Element : {first_element}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_E9wa9H__QH","executionInfo":{"status":"ok","timestamp":1734462457567,"user_tz":-120,"elapsed":3,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"8fa3f7a5-2573-40e8-da61-270dffaf05cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Flatten Tensor : tensor([ 1,  2,  3, -1,  5,  6, 17,  8,  9])\n","First Element : 1\n"]}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Random tensors.","metadata":{"id":"N0KfOLcC9hRG"}},{"cell_type":"code","source":"# Create a float32 random tensor from [0,1] of size (3, 4)\nrandom_tensor = torch.rand(size = (3,4) ,dtype = torch.float32)\nrandom_tensor , random_tensor.ndim , random_tensor.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltWF5a3n3r4p","executionInfo":{"status":"ok","timestamp":1734462580498,"user_tz":-120,"elapsed":315,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"8d2e7a53-886f-4d50-a8c0-ea62c17aac09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.3493, 0.1509, 0.7322, 0.7512],\n","         [0.9953, 0.8399, 0.1716, 0.3287],\n","         [0.9329, 0.1988, 0.8565, 0.3763]]),\n"," 2,\n"," torch.Size([3, 4]))"]},"metadata":{},"execution_count":34}],"execution_count":null},{"cell_type":"code","source":"# Create a float64 random tensor from [0,1] of size (3, 4)\nrandom_tensor1 = torch.rand(size = (3,4) ,dtype = torch.float64)\n\nrandom_tensor1 , random_tensor1.ndim , random_tensor1.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOMnqBsc5BZv","executionInfo":{"status":"ok","timestamp":1734462584960,"user_tz":-120,"elapsed":6,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"14622d24-82a1-4884-8754-79e994308ff7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.7192, 0.2307, 0.3309, 0.7059],\n","         [0.8675, 0.9852, 0.4502, 0.0688],\n","         [0.5937, 0.9873, 0.2284, 0.0930]], dtype=torch.float64),\n"," 2,\n"," torch.Size([3, 4]))"]},"metadata":{},"execution_count":35}],"execution_count":null},{"cell_type":"code","source":"# Create a intger8/16/32/64 random tensor from [low,high] of size (3, 4)\nintger_random_tensor = torch.randint(low=0 ,high=10\n\n                                     ,size=(3,4) ,dtype = torch.int64)\n\nintger_random_tensor , intger_random_tensor.ndim , intger_random_tensor.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NRKkqZu5Lkv","executionInfo":{"status":"ok","timestamp":1734462611838,"user_tz":-120,"elapsed":291,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"1bdbc35e-3273-4852-a6dd-1882b2ff050f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[2, 0, 2, 5],\n","         [0, 9, 0, 6],\n","         [5, 1, 5, 6]]),\n"," 2,\n"," torch.Size([3, 4]))"]},"metadata":{},"execution_count":36}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Zeros and Ones.","metadata":{"id":"_vPSS1DU9cb_"}},{"cell_type":"code","source":"# Create a tensor of all zeros\nzeros = torch.zeros(size = (3,4))\n\nzeros ,zeros.dtype","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eh6R1lP65r07","executionInfo":{"status":"ok","timestamp":1732317420742,"user_tz":-120,"elapsed":33,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"b3da7bb0-ffab-4027-c3ed-575ef56f5d20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]),\n"," torch.float32)"]},"metadata":{},"execution_count":56}],"execution_count":null},{"cell_type":"code","source":"# Create a tensor of all ones\nones = torch.ones(size = (3,4))\n\nones , ones.dtype ,ones.device","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2YP6JIq58nM","executionInfo":{"status":"ok","timestamp":1732317420742,"user_tz":-120,"elapsed":32,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"3f6d999b-7c9d-480c-8335-df2cf2575d08"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]),\n"," torch.float32,\n"," device(type='cpu'))"]},"metadata":{},"execution_count":57}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Createing a range tensors and tensors-like.","metadata":{"id":"MFWUeqiM9xVK"}},{"cell_type":"code","source":"# Use torch.arange(), torch.range() is deprecated\nzero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\nprint(zero_to_ten_deprecated)\n\n# Create a range of values 0 to 10\nzero_to_ten = torch.arange(start = 0, end = 10 ,step = 1)\nprint(zero_to_ten)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2hdrcOQ6G7f","executionInfo":{"status":"ok","timestamp":1732317420742,"user_tz":-120,"elapsed":30,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"20edc3d5-8d57-42c2-8d97-23e7b71b58d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n","tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-58-2ce73a16bcd8>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n"]}],"execution_count":null},{"cell_type":"code","source":"# Creating tensors zeros-like\nzeros_like = torch.zeros_like(input = zero_to_ten)\nzeros_like","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m12wF4Ic-5gD","executionInfo":{"status":"ok","timestamp":1732317420742,"user_tz":-120,"elapsed":28,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"e8162cb0-70ea-4ed1-c087-32ffa5addc4e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"metadata":{},"execution_count":59}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Tensors operations.","metadata":{"id":"HrCe92sDA1b5"}},{"cell_type":"code","source":"tensor = torch.tensor([1,2,3])\ntensor","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hi4TOVzLA4xb","executionInfo":{"status":"ok","timestamp":1734463505021,"user_tz":-120,"elapsed":281,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"9a861d68-79e4-4477-a408-846c76269b13"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3])"]},"metadata":{},"execution_count":42}],"execution_count":null},{"cell_type":"code","source":"# add 10 to tensor\nAddition = tensor + 10  # Or\naddition = torch.add(tensor , 10)\n\nAddition , addition","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hDeQlcCA-DB","executionInfo":{"status":"ok","timestamp":1734463507519,"user_tz":-120,"elapsed":4,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"59f058cb-55c4-446b-e654-850791870ced"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([11, 12, 13]), tensor([11, 12, 13]))"]},"metadata":{},"execution_count":43}],"execution_count":null},{"cell_type":"code","source":"Sub = tensor - 1 #Or\nsub = torch.sub(tensor ,1)\n\nSub , sub","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1RFX6oGBRKh","executionInfo":{"status":"ok","timestamp":1734463510799,"user_tz":-120,"elapsed":331,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"50913966-a962-4ab3-9086-338019fed5d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 1, 2]), tensor([0, 1, 2]))"]},"metadata":{},"execution_count":44}],"execution_count":null},{"cell_type":"code","source":"Mul = tensor  *11  #Or\nmul = torch.mul(tensor , 11)\n\nMul  , mul","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sE1Bw_DXBkeH","executionInfo":{"status":"ok","timestamp":1734463514393,"user_tz":-120,"elapsed":333,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"2c4e443b-2e41-4717-c533-93faa3d12ed0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([11, 22, 33]), tensor([11, 22, 33]))"]},"metadata":{},"execution_count":45}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Matrix Multiplication.\n\n\n\nTwo main ways of performing muliplication in neural networks and deep learning:\n\n\n\n1. Element-wise multiplication\n\n2. Marix multiplication(dot product)","metadata":{"id":"P7tLgGk7CRsw"}},{"cell_type":"code","source":"# Create element wise multiplication\nprint(tensor ,\"*\" ,tensor)\nprint(\"Equals:\" ,tensor * tensor)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZL3llD_pByQL","executionInfo":{"status":"ok","timestamp":1734463518231,"user_tz":-120,"elapsed":2,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"19927426-9bea-45a3-9484-353292d10539"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) * tensor([1, 2, 3])\n","Equals: tensor([1, 4, 9])\n"]}],"execution_count":null},{"cell_type":"code","source":"# Create matrix multiplication\nmul_value = torch.matmul(tensor , tensor)\n\nmul_value","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X592kQ8FE2Ge","executionInfo":{"status":"ok","timestamp":1734463522875,"user_tz":-120,"elapsed":316,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"5f2aba63-1778-4c91-80d5-bd4ecbc649de"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":47}],"execution_count":null},{"cell_type":"markdown","source":"**Note**: let's see wall time when we use `loop` , `matmul` ,`mm` ,`bmm` , `@` in Matrix Multiplication.","metadata":{"id":"JwVAaoSkFdTx"}},{"cell_type":"code","source":"%%time\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\n\nvalue","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsCH_0abFEis","executionInfo":{"status":"ok","timestamp":1734463529191,"user_tz":-120,"elapsed":328,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"b5aafa9c-95d0-4cbd-be50-47a7420fc592"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1.66 ms, sys: 0 ns, total: 1.66 ms\n","Wall time: 1.8 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":48}],"execution_count":null},{"cell_type":"code","source":"%%time\nmat_value = torch.matmul(tensor ,tensor)\n\nmat_value","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fEvdql9FSTD","executionInfo":{"status":"ok","timestamp":1734463542304,"user_tz":-120,"elapsed":294,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"c8e3ed05-6055-443e-ed52-418d27a165bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 720 ¬µs, sys: 0 ns, total: 720 ¬µs\n","Wall time: 557 ¬µs\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":49}],"execution_count":null},{"cell_type":"code","source":"%%time\ntensor @ tensor","metadata":{"id":"NgKGHp7uGPwg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734463632536,"user_tz":-120,"elapsed":291,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"9ef52ae3-8565-4b40-ff8a-1094df02560b"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 180 ¬µs, sys: 0 ns, total: 180 ¬µs\n","Wall time: 191 ¬µs\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":50}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ One of the most common errors in deep learning (shape errors).  \n\nBecause much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors you'll run into in deep learning is shape mismatches.","metadata":{"id":"P4yeJ06IBimc"}},{"cell_type":"code","source":"# Shapes need to be in the right way\ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11],\n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n# That will return RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)","metadata":{"id":"M9idsFvvBkiQ","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"error","timestamp":1734463835136,"user_tz":-120,"elapsed":299,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"4cbb517c-845d-4c40-daf5-51ddf5863751"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-b0beef979b95>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                          [9, 12]], dtype=torch.float32)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# That will return RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"]}],"execution_count":null},{"cell_type":"markdown","source":"We can make matrix multiplication work between tensor_A and tensor_B by making their inner dimensions match.\n\n\n\nOne of the ways to do this is with a **transpose** (switch the dimensions of a given tensor).\n\n\n\nYou can perform transposes in PyTorch using either:\n\n\n\n* torch.transpose(input, dim0, dim1) - where input is the desired tensor to transpose and dim0 and dim1 are the dimensions to be swapped.\n\n* tensor.T - where tensor is the desired tensor to transpose.","metadata":{"id":"ECyfRAqmCHXK"}},{"cell_type":"code","source":"print(tensor_A.shape ,\" , \" ,tensor_B.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-h0ju4qCKW-","executionInfo":{"status":"ok","timestamp":1734463847842,"user_tz":-120,"elapsed":381,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"f01c1e4e-e3fb-4c4a-e9de-6c6f56946621"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 2])  ,  torch.Size([3, 2])\n"]}],"execution_count":null},{"cell_type":"code","source":"# View tensor_A and tensor_B\ntensor_A ,tensor_B","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlWpqq3ICd9K","executionInfo":{"status":"ok","timestamp":1734463850604,"user_tz":-120,"elapsed":321,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"1f5fd4cf-6480-4d82-faf3-70e42c090a3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2.],\n","         [3., 4.],\n","         [5., 6.]]),\n"," tensor([[ 7., 10.],\n","         [ 8., 11.],\n","         [ 9., 12.]]))"]},"metadata":{},"execution_count":53}],"execution_count":null},{"cell_type":"code","source":"# View tensor_A and tensor_B.T\ntensor_A ,tensor_B.T","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63SAh_LJCu-r","executionInfo":{"status":"ok","timestamp":1734463870486,"user_tz":-120,"elapsed":330,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"021adf8f-2102-4920-8cb7-415c8891400a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2.],\n","         [3., 4.],\n","         [5., 6.]]),\n"," tensor([[ 7.,  8.,  9.],\n","         [10., 11., 12.]]))"]},"metadata":{},"execution_count":54}],"execution_count":null},{"cell_type":"code","source":"# The operation works when tensor_B is transposed\n\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\nprint(\"Output:\\n\")\n\nOutput = tensor_A @ tensor_B.T\n#Output = torch.matmul(tensor_A ,tensor_B.T)\nprint(Output)\nprint(f\"\\nOutput Shape: {Output.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22TorGsOCzcH","executionInfo":{"status":"ok","timestamp":1734463878151,"user_tz":-120,"elapsed":309,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"0d1595aa-41da-4f28-cd00-ebac4966ffe9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n","\n","New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n","\n","Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n","\n","Output:\n","\n","tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])\n","\n","Output Shape: torch.Size([3, 3])\n"]}],"execution_count":null},{"cell_type":"markdown","source":"**Note:** You can also use torch.mm() which is a short for torch.matmul().\n\n* Use torch.mm() only for 2D tensors (matrices).\n\n* For batch matrix multiplication of 3D tensors, use torch.bmm().\n\n* sure the dimensions of the tensors are compatible for matrix multiplication:\n\n   - If **A** is (ùëö,ùëõ), and **B** is (ùëõ,ùëù), the result will be (ùëö,ùëù).","metadata":{"id":"HpmuUVhgDVT3"}},{"cell_type":"code","source":"torch.mm(tensor_A,tensor_B.T)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nw9IbclPDpDR","executionInfo":{"status":"ok","timestamp":1734464330549,"user_tz":-120,"elapsed":300,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"0de2daa5-d181-45bd-c377-656d066a968a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])"]},"metadata":{},"execution_count":57}],"execution_count":null},{"cell_type":"markdown","source":"**Or we can use first_tensor.mm(sencod_tensor)**","metadata":{"id":"m4jnkkzFEC39"}},{"cell_type":"code","source":"tensor_A.mm(tensor_B.T)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aH6LL9CwDU9d","executionInfo":{"status":"ok","timestamp":1734464339543,"user_tz":-120,"elapsed":350,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"466773df-b7dd-4ee9-fe24-9216055a7c6c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])"]},"metadata":{},"execution_count":58}],"execution_count":null},{"cell_type":"code","source":"# If you have 3D tensors and need to perform batch matrix multiplication, use torch.bmm() instead. Here‚Äôs an example:\n\ntensors_3d = torch.tensor([\n                         [[1, 2], [3, 4]],\n                         [[5, 6], [7, 8]] ])  # A batch of 2 matrices (2x2 each)\n\n# Perform batch matrix multiplication\nresult = torch.bmm(tensors_3d, tensors_3d)\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trdMl0K4IeXr","executionInfo":{"status":"ok","timestamp":1734464408563,"user_tz":-120,"elapsed":310,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"7cb01991-aae8-48c2-f4aa-466c524c55d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[  7,  10],\n","         [ 15,  22]],\n","\n","        [[ 67,  78],\n","         [ 91, 106]]])\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Finding the min, max, mean, sum, etc (tensor aggregation).","metadata":{"id":"vw4d7_miGeiY"}},{"cell_type":"code","source":"x = torch.arange(1,100,10)\nx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFvO9MKBElvJ","executionInfo":{"status":"ok","timestamp":1734464462942,"user_tz":-120,"elapsed":334,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"8d411080-c4eb-4a11-e4d9-9642afb76e97"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])"]},"metadata":{},"execution_count":61}],"execution_count":null},{"cell_type":"code","source":"# Find the min in tensor\ntorch.min(x) , x.min() ,x.min().item() # Find minimum as a single value by use .item() # Output: 1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdbfEjgHGtU9","executionInfo":{"status":"ok","timestamp":1734464487252,"user_tz":-120,"elapsed":813,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"4f9daefe-c801-4318-af22-bc47d84bd86b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(1), tensor(1), 1)"]},"metadata":{},"execution_count":62}],"execution_count":null},{"cell_type":"code","source":"# Find the max in tensor\ntorch.max(x) , x.max() , x.max().item() # Find maximum as a single value by use .item() # Output: 91","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qocGLcjMGzUl","executionInfo":{"status":"ok","timestamp":1734464606813,"user_tz":-120,"elapsed":323,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"0db8724a-47d9-4240-df25-5bd7a9d03d78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(91), tensor(91), 91)"]},"metadata":{},"execution_count":63}],"execution_count":null},{"cell_type":"markdown","source":"**_`torch.mean(x)`_**\n\n>Note: You may find some methods such as torch.mean() require tensors to be in **torch.float32** (the most common) or another specific datatype, otherwise the operation will fail.\n","metadata":{"id":"5mZJSI2rHQyU"}},{"cell_type":"code","source":"# Find the mean\ntorch.mean(x.type(torch.float32)) ,x.type(torch.float32).mean()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rg0GSFyNG-cb","executionInfo":{"status":"ok","timestamp":1734464894329,"user_tz":-120,"elapsed":362,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"9bdcf8ee-3185-4374-977b-d44cd3d42405"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(46.), tensor(46.))"]},"metadata":{},"execution_count":66}],"execution_count":null},{"cell_type":"code","source":"print(x.to(torch.float64).mean().item()) # Or\nprint(x.float().mean().item())           # Find mean as a single value by use .item() Output: 46.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uCnuZuJKgAd","executionInfo":{"status":"ok","timestamp":1734465022352,"user_tz":-120,"elapsed":296,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"a602a0d5-58a8-45e3-adb3-1ccdf4272314"},"outputs":[{"output_type":"stream","name":"stdout","text":["46.0\n","46.0\n"]}],"execution_count":null},{"cell_type":"code","source":"# Find the sum\ntorch.sum(x) , x.sum() ,x.sum().item()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWfzGDiPHtuD","executionInfo":{"status":"ok","timestamp":1734465052012,"user_tz":-120,"elapsed":341,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"6a085f82-d5d4-4d28-eb9f-196aa4ba559a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(460), tensor(460), 460)"]},"metadata":{},"execution_count":73}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Finding The position of min ,max values in tensor.","metadata":{"id":"z2tC_Oj1I-oN"}},{"cell_type":"code","source":"x","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qwnj2Jg8HzKN","executionInfo":{"status":"ok","timestamp":1734465063400,"user_tz":-120,"elapsed":280,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"ddb6d3ad-df42-4c20-f20f-cb601145aba5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])"]},"metadata":{},"execution_count":74}],"execution_count":null},{"cell_type":"code","source":"# Find the position in tensor that has the minmum vlaue with argmin()\nmin_indx = x.argmin()\nprint(f\"Min value at index : {min_indx}\")\nprint(f\"\\nMin vlaue : {x[min_indx]}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vN7BnLjLJM60","executionInfo":{"status":"ok","timestamp":1734465070029,"user_tz":-120,"elapsed":335,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"176259a4-4044-46ba-f24e-9f85428491ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Min value at index : 0\n","\n","Min vlaue : 1\n"]}],"execution_count":null},{"cell_type":"code","source":"# Find the position in tensor that has the maxmum vlaue with argmax()\nmax_indx = x.argmax()\nprint(f\"Max value at index : {max_indx}\")\nprint(f\"\\nMax vlaue : {x[max_indx]}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhyuNWC5JtdF","executionInfo":{"status":"ok","timestamp":1734465084484,"user_tz":-120,"elapsed":317,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"c93849f6-b8a2-434c-d0f4-c0b634d72d00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Max value at index : 9\n","\n","Max vlaue : 91\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Reshaping, stacking, squeezing and unsqueezing ,permutation.\n\n|Method| One-line description\n\n|:----------:|:----------:\n\n|torch.reshape(input, shape)|\tReshapes `input` to `shape` (if compatible), can also use `torch.Tensor.reshape()`.  \n\n|Tensor.view(shape)|\tReturns a view of the original tensor in a different shape but shares the same data as the original tensor.  \n\n|torch.stack(tensors, dim=0)|\tConcatenates a sequence of tensors along a new dimension (dim), all tensors must be same size.  \n\n|torch.squeeze(input)|\tSqueezes input to remove all the dimenions with value `1`.  \n\n|torch.unsqueeze(input, dim)|\tReturns `input` with a dimension value of `1` added at `dim`.  \n\n|torch.permute(input, dims)|\tReturns a view of the original input with its dimensions permuted (rearranged) to dims.","metadata":{"id":"4XYHKRzZaoVb"}},{"cell_type":"code","source":"# Create a tensor\nt = torch.arange(1,10)\n\nt ,t.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0B9esFaoKAgU","executionInfo":{"status":"ok","timestamp":1734465744214,"user_tz":-120,"elapsed":391,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"564d06c3-5ff7-48de-bfbc-f2fda1c5d4d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]), torch.Size([9]))"]},"metadata":{},"execution_count":79}],"execution_count":null},{"cell_type":"code","source":"# Add extra dimension but must eqaul (orignal size)\nt_reshaped = t.reshape(1,9)\nt_reshaped , t_reshaped.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJC7pUu5dDhK","executionInfo":{"status":"ok","timestamp":1734465747234,"user_tz":-120,"elapsed":438,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"16a30d21-ade4-4dc3-9425-61714fa9c48d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]]), torch.Size([1, 9]))"]},"metadata":{},"execution_count":80}],"execution_count":null},{"cell_type":"code","source":"t_reshaped1 = t.reshape(9,1)\nt_reshaped1 , t_reshaped1.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLC36CNZdeIp","executionInfo":{"status":"ok","timestamp":1734465748900,"user_tz":-120,"elapsed":401,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"02cd586d-bdc9-4394-82ee-a013881a1c98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1],\n","         [2],\n","         [3],\n","         [4],\n","         [5],\n","         [6],\n","         [7],\n","         [8],\n","         [9]]),\n"," torch.Size([9, 1]))"]},"metadata":{},"execution_count":81}],"execution_count":null},{"cell_type":"code","source":"# Change the view\nz = t.view(1,9)\nz ,z.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4IsPOkCdjPT","executionInfo":{"status":"ok","timestamp":1734465751933,"user_tz":-120,"elapsed":318,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"32c25436-3ce7-486c-c00c-009b759a7d26"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]]), torch.Size([1, 9]))"]},"metadata":{},"execution_count":82}],"execution_count":null},{"cell_type":"code","source":"# Changing z changes t (because a view of a tensor shares the same data as the original input)\nz[:,0] = 99\nz ,t","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-7cDGUVdzH4","executionInfo":{"status":"ok","timestamp":1734465753548,"user_tz":-120,"elapsed":307,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"a3722c56-5992-491c-876e-ae938d6a5774"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[99,  2,  3,  4,  5,  6,  7,  8,  9]]),\n"," tensor([99,  2,  3,  4,  5,  6,  7,  8,  9]))"]},"metadata":{},"execution_count":83}],"execution_count":null},{"cell_type":"code","source":"# Stack tensors on top of each other\n# torch.stack(input can by taple Or list)\nt_stacked  = torch.stack([t,t,t] ,dim = 0)\nt_stacked ,t_stacked.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfRZHFSdeaH_","executionInfo":{"status":"ok","timestamp":1734465757945,"user_tz":-120,"elapsed":282,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"816c6a30-ce43-4efb-b2f0-92dfe76472b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[99,  2,  3,  4,  5,  6,  7,  8,  9],\n","         [99,  2,  3,  4,  5,  6,  7,  8,  9],\n","         [99,  2,  3,  4,  5,  6,  7,  8,  9]]),\n"," torch.Size([3, 9]))"]},"metadata":{},"execution_count":84}],"execution_count":null},{"cell_type":"code","source":"t_stacked1  = torch.stack([t,t,t] ,dim = 1)\nt_stacked1 ,t_stacked1.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqiWmBLoe5Qu","executionInfo":{"status":"ok","timestamp":1734465780191,"user_tz":-120,"elapsed":317,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"ba03c6eb-6f76-4427-a0b8-4d1045f1cb8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[99, 99, 99],\n","         [ 2,  2,  2],\n","         [ 3,  3,  3],\n","         [ 4,  4,  4],\n","         [ 5,  5,  5],\n","         [ 6,  6,  6],\n","         [ 7,  7,  7],\n","         [ 8,  8,  8],\n","         [ 9,  9,  9]]),\n"," torch.Size([9, 3]))"]},"metadata":{},"execution_count":85}],"execution_count":null},{"cell_type":"markdown","source":"**` torch.vstack() `**  \n\n* Purpose: Vertically stacks tensors along a new first dimension (dim=0).\n\n* Input Requirement: Tensors must have the same shape, except for the first dimension.\n\n* Shape Change:\n\n   - If tensors have shape `(x, y)`, the result will have shape `(x1 + x2, y)`, where `x1` and `x2` are the first dimensions of the input tensors.","metadata":{"id":"V0WEcwVvgCKW"}},{"cell_type":"code","source":"a = torch.tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\nb = torch.tensor([[5, 6]])          # Shape: (1, 2)\n\nresult = torch.vstack((a, b))      # Shape: (3, 2)\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSi5l58afCL2","executionInfo":{"status":"ok","timestamp":1732415906668,"user_tz":-120,"elapsed":19,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"ff305ff9-b666-435b-f4a3-09d45e19dc1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n"]}],"execution_count":null},{"cell_type":"markdown","source":"**` torch.hstack() `**  \n\n* Purpose: Horizontally stacks tensors along the last dimension (dim=1).\n\n* Input Requirement: Tensors must have the same shape, except for the last dimension.\n\n* Shape Change:\n\n  - If tensors have shape `(x, y)`, the result will have shape `(x, y1 + y2)`, where `y1` and `y2` are the second dimensions of the input tensors.","metadata":{"id":"E1bRlyxZgls2"}},{"cell_type":"code","source":"a = torch.tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\nb = torch.tensor([[5], [6]])        # Shape: (2, 1)\n\nresult = torch.hstack((a, b))       # Shape: (2, 3)\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHg1utVigifE","executionInfo":{"status":"ok","timestamp":1732415906668,"user_tz":-120,"elapsed":17,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"a9006f3d-1333-4bc8-84eb-ebc5b7aecf63"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2, 5],\n","        [3, 4, 6]])\n"]}],"execution_count":null},{"cell_type":"code","source":"t","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DY9g_BmrlpLs","executionInfo":{"status":"ok","timestamp":1732415906668,"user_tz":-120,"elapsed":15,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"0e612352-9796-42f2-a1a8-e34ca65b6992"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([99,  2,  3,  4,  5,  6,  7,  8,  9])"]},"metadata":{},"execution_count":22}],"execution_count":null},{"cell_type":"code","source":"# How about removing all single dimensions from a tensor?\n# To do so you can use torch.squeeze() (I remember this as squeezing the tensor to only have dimensions over 1).\nprint(f\"Previous tensor: {t_reshaped}\")\nprint(f\"Previous shape: {t_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nt_squeezed = t_reshaped.squeeze()\nprint(f\"\\nNew tensor: {t_squeezed}\")\nprint(f\"New shape: {t_squeezed.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQTvKNpOhTLc","executionInfo":{"status":"ok","timestamp":1732415906668,"user_tz":-120,"elapsed":13,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"01864e80-3c67-4ab5-d2ed-80bf09e4c5b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([[99,  2,  3,  4,  5,  6,  7,  8,  9]])\n","Previous shape: torch.Size([1, 9])\n","\n","New tensor: tensor([99,  2,  3,  4,  5,  6,  7,  8,  9])\n","New shape: torch.Size([9])\n"]}],"execution_count":null},{"cell_type":"code","source":"# To do the reverse of torch.squeeze() you can use torch.unsqueeze() to add a dimension value of 1 at a specific index.\nprint(f\"Previous tensor: {t_squeezed}\")\nprint(f\"Previous shape: {t_squeezed.shape}\")\n## Add an extra dimension with unsqueeze\n\nt_unsqueezed = t_squeezed.unsqueeze(dim=0)\nprint(f\"\\nNew tensor: {t_unsqueezed}\")\nprint(f\"New shape: {t_unsqueezed.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csOyU81elrg0","executionInfo":{"status":"ok","timestamp":1732415906668,"user_tz":-120,"elapsed":11,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"2d674840-65ec-4a05-d19e-b8be52549882"},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous tensor: tensor([99,  2,  3,  4,  5,  6,  7,  8,  9])\n","Previous shape: torch.Size([9])\n","\n","New tensor: tensor([[99,  2,  3,  4,  5,  6,  7,  8,  9]])\n","New shape: torch.Size([1, 9])\n"]}],"execution_count":null},{"cell_type":"markdown","source":"You can also rearrange the order of axes values with torch.permute(input, dims), where the input gets turned into a view with new dims.\n\n>Note: Because permuting returns a view (shares the same data as the original), the values in the permuted tensor will be the same as the original tensor and if you change the values in the view, it will change the values of the original.","metadata":{"id":"8oiLa6Hul_Go"}},{"cell_type":"code","source":"# Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n# Permute the original tensor to rearrange the axis order\n\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n# Or x_permuted = torch.permute(x_original ,(2,0,1))\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLnpsG4Fl5jM","executionInfo":{"status":"ok","timestamp":1732415906668,"user_tz":-120,"elapsed":8,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"24bc07bb-7c85-498c-d28d-9dd08afa2e51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous shape: torch.Size([224, 224, 3])\n","New shape: torch.Size([3, 224, 224])\n"]}],"execution_count":null},{"cell_type":"code","source":"x_original[0,0,0] = 6987","metadata":{"id":"EIWLGugBBIpT"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_original[0,0,0] , x_permuted[0,0,0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeXWUVQQDTgk","executionInfo":{"status":"ok","timestamp":1732416215942,"user_tz":-120,"elapsed":280,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"d9f1e016-ca5c-4985-ed28-a5753a858101"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(6987.), tensor(6987.))"]},"metadata":{},"execution_count":33}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Indexing (selecting data from tensors).","metadata":{"id":"qWoMeTOKF9PZ"}},{"cell_type":"code","source":"mat = torch.arange(1,10).reshape(1,3,3)\n\nmat","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EZO-xMTGASc","executionInfo":{"status":"ok","timestamp":1732422139200,"user_tz":-120,"elapsed":281,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"e0e066a6-b794-44e3-a7d5-7af321a99f93"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1, 2, 3],\n","         [4, 5, 6],\n","         [7, 8, 9]]])"]},"metadata":{},"execution_count":68}],"execution_count":null},{"cell_type":"code","source":"mat[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkMk1JMvGRAY","executionInfo":{"status":"ok","timestamp":1732422140805,"user_tz":-120,"elapsed":1032,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"05253b2a-0f65-4906-ec1b-6ce2289caff0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])"]},"metadata":{},"execution_count":69}],"execution_count":null},{"cell_type":"code","source":"mat[0][2][2].item() , mat[:,2,2]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zt0CZNVyaJxR","executionInfo":{"status":"ok","timestamp":1732422430913,"user_tz":-120,"elapsed":286,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"089b0a66-8248-4e58-f305-392361076194"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(9, tensor([9]))"]},"metadata":{},"execution_count":88}],"execution_count":null},{"cell_type":"code","source":"mat[0][0] ,mat[0][1] ,mat[0][2]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2AJyVzDGQ4l","executionInfo":{"status":"ok","timestamp":1732422140806,"user_tz":-120,"elapsed":12,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"69769f58-e9b5-469a-85a8-250f15228ef7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))"]},"metadata":{},"execution_count":70}],"execution_count":null},{"cell_type":"code","source":"mat[0][0][0] ,mat[0][0][1] ,mat[0,0,2] ,mat[:,0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJtkuzB6GQ06","executionInfo":{"status":"ok","timestamp":1732422140806,"user_tz":-120,"elapsed":10,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"92057174-0681-487f-db1f-03264591a6cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(1), tensor(2), tensor(3), tensor([[1, 2, 3]]))"]},"metadata":{},"execution_count":71}],"execution_count":null},{"cell_type":"code","source":"mat[0][1][0] ,mat[0][1][1] ,mat[0][1][2] ,mat[:,1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqykdzLtGQqv","executionInfo":{"status":"ok","timestamp":1732422140806,"user_tz":-120,"elapsed":8,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"39719b2b-1f80-4ede-f205-66d8a53e5faf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(4), tensor(5), tensor(6), tensor([[4, 5, 6]]))"]},"metadata":{},"execution_count":72}],"execution_count":null},{"cell_type":"code","source":"mat[0][2][0] ,mat[0][2][1] ,mat[0][2][2] ,mat[:,2]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdCojhC6GQZs","executionInfo":{"status":"ok","timestamp":1732422140806,"user_tz":-120,"elapsed":6,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"918cf7e5-5bbc-4835-82af-457de12b0056"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(7), tensor(8), tensor(9), tensor([[7, 8, 9]]))"]},"metadata":{},"execution_count":73}],"execution_count":null},{"cell_type":"code","source":"mat[:,:,0:0] ,mat[:,:,0:1] ,mat[:,:,0:2] ,mat[:,:,0:3]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3Fd_cTbIAJl","executionInfo":{"status":"ok","timestamp":1732422140806,"user_tz":-120,"elapsed":5,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"d49d8427-0cd7-4a0c-e4d7-f561b8eeb77f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([], size=(1, 3, 0), dtype=torch.int64),\n"," tensor([[[1],\n","          [4],\n","          [7]]]),\n"," tensor([[[1, 2],\n","          [4, 5],\n","          [7, 8]]]),\n"," tensor([[[1, 2, 3],\n","          [4, 5, 6],\n","          [7, 8, 9]]]))"]},"metadata":{},"execution_count":74}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç PyTorch tensors & NumPy.\n\nSince NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.\n\n\n\nThe two main methods you'll want to use for NumPy to PyTorch (and back again) are:\n\n\n\n* torch.from_numpy(ndarray) - NumPy array -> PyTorch tensor.\n\n* torch.Tensor.numpy() - PyTorch tensor -> NumPy array.  \n\n* **Let's try them out.**","metadata":{"id":"BT8g56u8dESA"}},{"cell_type":"code","source":"import torch\nimport numpy as np\narray = np.arange(1.0 ,8.0)\n\narray","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9rvBsNFnaE0b","executionInfo":{"status":"ok","timestamp":1732423476353,"user_tz":-120,"elapsed":256,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"e1996fa2-25b3-46c4-e22a-0dc4fc931127"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 2., 3., 4., 5., 6., 7.])"]},"metadata":{},"execution_count":96}],"execution_count":null},{"cell_type":"code","source":"numpy_to_tensor = torch.from_numpy(array)\narray ,numpy_to_tensor","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSeW1h7AaExl","executionInfo":{"status":"ok","timestamp":1732423479815,"user_tz":-120,"elapsed":317,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"76966a3b-ce5b-4dc7-e1ee-0a2bf145bbaf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([1., 2., 3., 4., 5., 6., 7.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"metadata":{},"execution_count":97}],"execution_count":null},{"cell_type":"markdown","source":">Note: By default, NumPy arrays are created with the datatype float64 and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above).\n\n\n\n>However, many PyTorch calculations default to using float32.\n\n\n\n>So if you want to convert your NumPy array (float64) -> PyTorch tensor (float64) -> PyTorch tensor (float32), you can use\n\n**`tensor = torch.from_numpy(array).type(torch.float32).`**","metadata":{"id":"XFi16fR3dumc"}},{"cell_type":"code","source":"# Change the values in array ,what will do to `tensor`? -> will not change (Not shared memory).\narray = array + 5\narray  ,numpy_to_tensor","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_R4YAXHuaEol","executionInfo":{"status":"ok","timestamp":1732423484781,"user_tz":-120,"elapsed":273,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"afa39475-f72d-4118-af5f-7dd3b9fa897d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 6.,  7.,  8.,  9., 10., 11., 12.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"metadata":{},"execution_count":98}],"execution_count":null},{"cell_type":"code","source":"tensor_zeros_num = torch.zeros(8)\ntensor_to_numpy  = tensor_zeros_num.numpy()\ntensor_zeros_num ,tensor_to_numpy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62Hxmy-waEaA","executionInfo":{"status":"ok","timestamp":1732423530073,"user_tz":-120,"elapsed":267,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"42b7b4f9-c1e2-4f9c-f4dd-7d76b69af553"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0., 0., 0., 0., 0., 0., 0., 0.]),\n"," array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"]},"metadata":{},"execution_count":99}],"execution_count":null},{"cell_type":"code","source":"# Change the values in tensor ,what will do to `array`? -> will not change (Not shared memory).\ntensor_zeros_num = tensor_zeros_num +5\ntensor_zeros_num ,tensor_to_numpy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fpfU7wtaERg","executionInfo":{"status":"ok","timestamp":1732423607361,"user_tz":-120,"elapsed":290,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"bd73f6d4-d97d-4fb9-916f-804f7df0f344"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([5., 5., 5., 5., 5., 5., 5., 5.]),\n"," array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"]},"metadata":{},"execution_count":100}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Reproducibility (trying to take the random out of random).\n\nIn short how neural networks learns:\n\n\n\n`Start with random numbers -> tensor operations -> try to make better again -> again -> again`","metadata":{"id":"AvvV9U5micMM"}},{"cell_type":"code","source":"import torch\nrandom_tensor_A = torch.rand(3,4)\nrandom_tensor_B = torch.rand(3,4)\n\nprint(random_tensor_A,\"\\n\")\nprint(random_tensor_B)\nprint(f\"\\nDoes Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sukS3vFi-h9","executionInfo":{"status":"ok","timestamp":1732424703428,"user_tz":-120,"elapsed":279,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"bfcab851-124c-41e8-bab6-9ad6626ce078"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0516, 0.6828, 0.2364, 0.4904],\n","        [0.5408, 0.2574, 0.4718, 0.5495],\n","        [0.4136, 0.4115, 0.8356, 0.7439]]) \n","\n","tensor([[0.0388, 0.6665, 0.0781, 0.9745],\n","        [0.6288, 0.7835, 0.9366, 0.4459],\n","        [0.9306, 0.1452, 0.1857, 0.4114]])\n","\n","Does Tensor A equal Tensor B? (anywhere)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[False, False, False, False],\n","        [False, False, False, False],\n","        [False, False, False, False]])"]},"metadata":{},"execution_count":107}],"execution_count":null},{"cell_type":"code","source":"# Set random seed\nRANDOM_SEED = 42\ntorch.manual_seed(RANDOM_SEED)\n\nrandom_tensor_C = torch.rand(4,3)\nrandom_tensor_D = torch.rand(4,3)\n\nprint(random_tensor_C,\"\\n\")\nprint(random_tensor_D)\nprint(f\"\\nDoes Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_C == random_tensor_D","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fq874eqbaEOI","executionInfo":{"status":"ok","timestamp":1732424831688,"user_tz":-120,"elapsed":304,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"95f4c9b8-1f5a-42d0-e199-9f48e2f1e31f"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009],\n","        [0.2566, 0.7936, 0.9408],\n","        [0.1332, 0.9346, 0.5936]]) \n","\n","tensor([[0.8694, 0.5677, 0.7411],\n","        [0.4294, 0.8854, 0.5739],\n","        [0.2666, 0.6274, 0.2696],\n","        [0.4414, 0.2969, 0.8317]])\n","\n","Does Tensor A equal Tensor B? (anywhere)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[False, False, False],\n","        [False, False, False],\n","        [False, False, False],\n","        [False, False, False]])"]},"metadata":{},"execution_count":108}],"execution_count":null},{"cell_type":"code","source":"# WE Must Set random seed before each random tensor\n\nRANDOM_SEED = 42\ntorch.manual_seed(RANDOM_SEED)\nrandom_tensor_E = torch.rand(4,3)\n\ntorch.manual_seed(RANDOM_SEED)\nrandom_tensor_F = torch.rand(4,3)\n\nprint(random_tensor_E,\"\\n\")\nprint(random_tensor_F)\nprint(f\"\\nDoes Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_E == random_tensor_F","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qIPL9RUSaEKP","executionInfo":{"status":"ok","timestamp":1732424894387,"user_tz":-120,"elapsed":320,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"71a57460-cbe3-486c-dd9c-03524379edbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009],\n","        [0.2566, 0.7936, 0.9408],\n","        [0.1332, 0.9346, 0.5936]]) \n","\n","tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009],\n","        [0.2566, 0.7936, 0.9408],\n","        [0.1332, 0.9346, 0.5936]])\n","\n","Does Tensor A equal Tensor B? (anywhere)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True],\n","        [True, True, True],\n","        [True, True, True],\n","        [True, True, True]])"]},"metadata":{},"execution_count":109}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Running tensors on GPUs (and making faster computations).","metadata":{"id":"1EfpejhO9Bh9"}},{"cell_type":"markdown","source":"## üî∏ Getting s GPU.","metadata":{"id":"eXsvC-r19D65"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Ansvz76DaDUD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734226108047,"user_tz":-120,"elapsed":476,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"7c58365c-bcf2-459b-a818-d82717244b51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 15 01:28:26 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"execution_count":null},{"cell_type":"markdown","source":"## üî∏ Getting PyTorch to run on the GPU.","metadata":{"id":"sNJQoKV29Qnr"}},{"cell_type":"code","source":"import torch\n# Check for GPU access with PyTorch\ntorch.cuda.is_available()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOeLYrfM9PFr","executionInfo":{"status":"ok","timestamp":1734403160868,"user_tz":-120,"elapsed":3812,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"d6acb7ea-e585-487a-d992-56f2cfc063f1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"execution_count":null},{"cell_type":"markdown","source":"Let's create a device variable to store what kind of device is available.","metadata":{"id":"B7fKqPmP9vi7"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8Vbozs3p9xCH","executionInfo":{"status":"ok","timestamp":1734226720011,"user_tz":-120,"elapsed":661,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"0a11b9c2-f671-4f15-d7d1-dd9e0d8c77b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else 'cpu'\ndevice # when i turn off gpu","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8IIl_S74apUM","executionInfo":{"status":"ok","timestamp":1734402059358,"user_tz":-120,"elapsed":494,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"75b49c61-df89-4a8d-e098-0ab029462556"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"execution_count":null},{"cell_type":"markdown","source":"If the above output \"cuda\" it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output \"cpu\", our PyTorch code will stick with the CPU.\n\n\n\n>Note: In PyTorch, it's best practice to write [device agnostic code](https://pytorch.org/docs/main/notes/cuda.html#device-agnostic-code). This means code that'll run on CPU (always available) or GPU (if available).\n\n\n\nIf you want to do faster computing you can use a GPU but if you want to do much faster computing, you can use multiple GPUs.\n\n\n\nYou can count the number of GPUs PyTorch has access to using torch.cuda.device_count().","metadata":{"id":"Jd1awngx-Qu2"}},{"cell_type":"code","source":"# Count number of device\ntorch.cuda.device_count() # when i turn on gpu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJ-wLgZT-e_6","executionInfo":{"status":"ok","timestamp":1734226943266,"user_tz":-120,"elapsed":462,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"82b47726-7b73-4daf-9cd6-7e1a0f25271e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}],"execution_count":null},{"cell_type":"code","source":"# to know number of devices\ntorch.cuda.device_count() # when i turn off gpu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKc86mYra6xy","executionInfo":{"status":"ok","timestamp":1734402133874,"user_tz":-120,"elapsed":311,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"11665636-5ade-4c42-8d46-991b54f82e2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}],"execution_count":null},{"cell_type":"markdown","source":"Knowing the number of GPUs PyTorch has access to is helpful incase you wanted to run a specific process on one GPU and another process on another (PyTorch also has features to let you run a process across all GPUs).","metadata":{"id":"91Lmk1p7-4wG"}},{"cell_type":"markdown","source":"## üî∏ Putting tensors (and models) on GPU.\n\nYou can put tensors (and models, we'll see this later) on a specific device by calling to(device) on them. Where device is the target device you'd like the tensor (or model) to go to.\n\n\n\nWhy do this?\n\n\n\nGPUs offer far faster numerical computing than CPUs do and if a GPU isn't available, because of our **device agnostic code** (see above), it'll run on the CPU.\n\n\n\n>**Note**: Putting a tensor on GPU using `to(device)` `(e.g. some_tensor.to(device))` returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:\n\n\n\n`some_tensor = some_tensor.to(device)`\n\n\n\nLet's try creating a tensor and putting it on the GPU (if it's available).","metadata":{"id":"SlvSwLjydrho"}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntensor = torch.tensor([1,2,3])\n\n# Tensor Not On GPU by defult in cpu\ntensor ,tensor.device","metadata":{"id":"T4buscQ9--wy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734403224965,"user_tz":-120,"elapsed":628,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"06e96aa9-3792-40f8-cc72-76064ad2abb4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1, 2, 3]), device(type='cpu'))"]},"metadata":{},"execution_count":6}],"execution_count":null},{"cell_type":"code","source":"tensor_on_gpu = tensor.to(device)\n# it's return cuda and number of device (there are 1 gpu in my decive ,so it retrun 0 this number of first gpu)\ntensor_on_gpu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2g18Cy8fYfK","executionInfo":{"status":"ok","timestamp":1734403299910,"user_tz":-120,"elapsed":695,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"3e83360a-19c6-41c2-c2d2-0945a8b5f7f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":7}],"execution_count":null},{"cell_type":"markdown","source":"### üî∏ Moving Tensors back to CPU.\n\nIf tensor on GPU it can't transfrom it to Numpy by **tensor_name.cpu().numpy**","metadata":{"id":"ZR3_J5Qgf4Bx"}},{"cell_type":"code","source":"tensor_on_gpu.numpy()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"Q874fbkLf3rW","executionInfo":{"status":"error","timestamp":1734403496344,"user_tz":-120,"elapsed":631,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"a27bebb0-6d0c-4a20-81b5-4fe80d1a98d3"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e0c96c7436fd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}],"execution_count":null},{"cell_type":"code","source":"# To fix this issues we must first set tensor in cpu\n\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfa6L1HegSrj","executionInfo":{"status":"ok","timestamp":1734403604157,"user_tz":-120,"elapsed":453,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"af2565f3-7307-4185-d7aa-5732958457ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":10}],"execution_count":null},{"cell_type":"code","source":"# Not change\ntensor_on_gpu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmRWUYPPgsdm","executionInfo":{"status":"ok","timestamp":1734403614101,"user_tz":-120,"elapsed":458,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"c6a99b34-96e7-4e72-aac3-950c775e7a19"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":11}],"execution_count":null},{"cell_type":"markdown","source":"# ‚óç Exercises : [PyTorch Fundamentals Exercises and Solutions](https://github.com/Adnan-Alaref/Pytorch-Tuatorial/blob/main/00_pytorch_fundamentals_exercises.ipynb).","metadata":{"id":"eQTj5fLWX0-p"}},{"cell_type":"markdown","source":"<a id=\"Import\"></a>\n<p style=\"background-color: #000000; font-family: 'Verdana', sans-serif; color: #FFFFFF; font-size: 160%; text-align: center; border-radius: 25px; padding: 12px 20px; margin-top: 20px; border: 2px solid transparent; background-image: linear-gradient(black, black), linear-gradient(45deg, #FF00FF, #00FFFF, #FFFF00, #FF4500); background-origin: border-box; background-clip: content-box, border-box; box-shadow: 0px 4px 20px rgba(255, 105, 180, 0.8);\">\n   Thanks & Upvote ‚ù§Ô∏è</p>","metadata":{}}]}